# âœ‹ Sign Language to Text Conversion

A **Computer Vision and Deep Learningâ€“based system** that converts **sign language hand gestures into readable text** in real time using a webcam.  
This project aims to bridge the communication gap between **deaf / hard-of-hearing individuals** and non-sign language users.

---

## ğŸ“Œ Project Overview

Sign language is a visual language that uses **hand gestures, orientation, and movement** to convey meaning.  
This project uses **Computer Vision (CV)** and **Convolutional Neural Networks (CNNs)** to recognize **American Sign Language (ASL) alphabets (Aâ€“Z)** and convert them into text.

The system captures live video, detects hands, preprocesses the gesture image, and predicts the corresponding alphabet using a trained deep learning model.

---

## ğŸ¯ Objectives

- Detect and track hand gestures in real time  
- Recognize ASL alphabet gestures accurately  
- Convert detected gestures into text output  
- Build an accessible real-time sign language recognition system  

---

## ğŸ› ï¸ Tech Stack

| Category | Technology |
|--------|------------|
| Programming Language | Python |
| Computer Vision | OpenCV |
| Hand Tracking | MediaPipe, CVZone |
| Deep Learning Framework | TensorFlow, Keras |
| Numerical Computing | NumPy |
| Model Type | Convolutional Neural Network (CNN) |
| Input Device | Webcam |

---

## ğŸ§  How It Works (Workflow)

1. **Video Capture** â€“ Live frames captured using a webcam  
2. **Hand Detection & Tracking** â€“ MediaPipe + CVZone detect hand landmarks  
3. **Image Preprocessing** â€“ Cropping, resizing, background normalization  
4. **Gesture Recognition** â€“ CNN model predicts the sign  
5. **Text Output** â€“ Predicted alphabet displayed on screen  

---

## âœ¨ Features

- ğŸ”´ Real-time webcam-based detection  
- âœ‹ Accurate hand landmark tracking  
- ğŸ§  CNN-based gesture classification  
- ğŸ”¤ Supports ASL alphabets (Aâ€“Z)  
- ğŸ“¸ Dataset collection using live camera  
- âš¡ Fast and responsive predictions  

---

## ğŸ“‚ Project Structure

```bash
Sign-Language-To-Text/
â”œâ”€â”€ Data/                  # Collected gesture images
â”œâ”€â”€ Model/
â”‚   â””â”€â”€ keras_model.h5     # Trained CNN model
â”œâ”€â”€ datacollection.py      # Dataset collection script
â”œâ”€â”€ test.py                # Real-time prediction script
â”œâ”€â”€ requirements.txt       # Project dependencies
â””â”€â”€ README.md
```

---
## âš™ï¸ Setup & Installation

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/bhoomikagoel24/sign-language-to-text.git
cd sign-language-to-text
```

### 2ï¸âƒ£ Create virtual environment (Python 3.10 recommended)
```bash
py -3.10 -m venv venv
venv\Scripts\activate
```

### 3ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```

### â–¶ï¸ Running the Project
- **Dataset Collection**
```bash
python datacollection.py
```
Press S to save gesture images.

- **Gesture Recognition**
```bash
python test.py
```
Press ESC / Q to exit.

## ğŸ“Š Dataset

- Custom dataset collected using webcam
- Images of hand gestures representing ASL alphabets
- Preprocessed and normalized for CNN training

### Sample Dataset Preview

<p align="center">
  <img src="assets/dataPreview_1.jpg" width="280"/>
  <img src="assets/dataPreview_2.jpg" width="280"/>
  <img src="assets/dataPreview_3.jpeg" width="280"/>
</p>

## âš ï¸ Limitations

- Performance depends on lighting conditions
- Supports static alphabet gestures only
- Continuous sentence recognition not implemented

## ğŸš€ Future Enhancements

- Word and sentence-level recognition
- Continuous gesture detection
- Multilingual sign language support (ISL, BSL, etc.)
- Text-to-Speech integration

## ğŸ‘©â€ğŸ’» Author
~ Bhoomika Goel